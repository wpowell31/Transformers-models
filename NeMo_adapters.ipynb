{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from nemo.core import NeuralModule, ModelPT\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim: int=50):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = torch.nn.Linear(dim, dim)\n",
    "        self.ln = torch.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.ln(x)\n",
    "        return x\n",
    "\n",
    "class ResidualMLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim: int, num_layers:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList([MLP(dim) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        for layer in self.layers:\n",
    "            x = layers(x)\n",
    "            x = x + input\n",
    "            input = x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(ModelPT):\n",
    "\n",
    "    def __init__(self, cfg, trainer=None):\n",
    "        super().__init__(cfg, trainer=trainer)\n",
    "\n",
    "        self.encoder = instantiate(cfg.encoder) # Type ResidualMLP\n",
    "        self.decoder = instantiate(cfg.decoder) # Type ResidualMLP\n",
    "        self.projection = torch.nn.Linear(self.decoder.dim, cfg.out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.encoder(x)\n",
    "        z = self.decoder(y)\n",
    "        out = self.projection(z)\n",
    "        return out\n",
    "\n",
    "    def list_available_models(cls):\n",
    "        return []\n",
    "\n",
    "    def setup_training_data(train_data_config):\n",
    "        pass\n",
    "\n",
    "    def setup_validation_data(val_data_config):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classpath(cls):\n",
    "    return f'{cls.__module__}.{cls.__name__}'\n",
    "\n",
    "def get_model_config(dim=512):\n",
    "    config = OmegaConf.create(\n",
    "        {\n",
    "            'in_features': dim,\n",
    "            'out_features': 10,\n",
    "            'encoder': {'_target_': get_classpath(ResidualMLP), 'dim': dim, 'num_layers': 4},\n",
    "            'decoder': {'_target_': get_classpath(ResidualMLP), 'dim': dim, 'num_layers': 2},\n",
    "        }\n",
    "    )\n",
    "    return config\n",
    "\n",
    "dim = 512\n",
    "model_cfg = get_model_config(dim)\n",
    "model = SimpleModel(model_cfg)\n",
    "model.summarize()\n",
    "\n",
    "# Check if the forward pass works !\n",
    "with torch.no_grad():\n",
    "  input_data = torch.randn(8, dim)\n",
    "  out = model(input_data)\n",
    "  print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.core import adapter_mixins\n",
    "help(adapter_mixins.AdapterModuleMixin)\n",
    "\n",
    "class MLP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
